{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameshcodes/deep-learning-codes/blob/main/nlp_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULq1yZItgJdI"
      },
      "source": [
        ":<h1 style=\"font-weight:bold;\">$$\\text{NLP 101: Hands On with NLTK}$$</h1>\n",
        "\n",
        "---\n",
        "$$\\text{Natural language processing (NLP) is the ability of a computer program to understand human language}$$\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> **What all tools we need for NLP ?**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**NLP 101: Text Preprocessing 1 - Cleaning the input**\n",
        "- [Tokenization](#sec-1)\n",
        "- [Stemming](#sec-2)\n",
        "- [Lemmatization](#sec-3)\n",
        "- [Stopwords](#sec-4)\n",
        "- [POS Tagging](#sec-5)\n",
        "- [Name-Entity Recognition](#sec-6)\n",
        "\n",
        "<br>\n",
        "\n",
        "**NLP 201: Text Prepocessing 2 - Basic(Input Text -> Vector)**\n",
        "- One hot Encoding\n",
        "- BOW\n",
        "- TF/IDF\n",
        "- Unigram, Bigram N-grams\n",
        "\n",
        "<br>\n",
        "\n",
        "**NLP 202: Text Prepocessing 3 - Advanced(Input Text -> Vector)**\n",
        "- Word-Embeddings\n",
        "- Word2Vec, Average Word2Vec\n",
        "- Glove\n",
        "\n",
        "<br>\n",
        "\n",
        "**NLP 301: Deep Learning - Basic(Modelling)**\n",
        "- RNN\n",
        "- LSTM\n",
        "- GRU\n",
        "<br>\n",
        "\n",
        "**NLP 302: Deep Learning - Advanced(Modelling)**\n",
        "- Encoder-Decoder\n",
        "- Transformers\n",
        "- Bert\n",
        "\n",
        "\n",
        "<br>\n",
        "<h2 style=\"font-weight:bold;\">$$\\text{Note: Here, We will be looking at NLP 101 tools only.}$$</h2>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcn3LdBRnQFG"
      },
      "source": [
        "$$\\text{Terminology in NLP}$$\n",
        "\n",
        "---\n",
        "\n",
        "- **Corpus**: A structured collection of texts used for NLP tasks. It is collection of Documents. ***for ex. Paragraph***\n",
        "- **Documents**: Individual units of text within a corpus. It represent a fact or an entity ***for ex. Sentences***\n",
        "- **Vocabulary**: The set of unique words in a corpus. ***for ex. Unique Words***\n",
        "\n",
        "<br>\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![NLP Terminology](https://miro.medium.com/v2/resize:fit:500/format:webp/1*jeKMXYsTtKlC2mzHIf17fQ.png)\n",
        "_Source: Medium_\n",
        "\n",
        "\n",
        "</div>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StLZCzNukyG0"
      },
      "source": [
        "<a id='sec-1'></a>\n",
        "## **Tokenization**\n",
        "\n",
        "$$\\text{Tokenization refers to the process of converting a sequence of text into smaller parts known as tokens}$$\n",
        "\n",
        "---\n",
        "We tokenize corpus into\n",
        "- [Sentences](#sec-11)\n",
        "- [Words](#sec-12)\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "$$\\textbf{NLTK Library}$$\n",
        "\n",
        "---\n",
        "**Installation**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaIka7WDkxhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30af2561-d3d4-4b0f-df71-b3dd39dae1b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjrUu8WorwoP"
      },
      "source": [
        "- NLTK, which stands for Natural Language Toolkit\n",
        "- It comes with easy-to-use tools to access lots of different types of language data, like WordNet.\n",
        "- NLTK also has helpful tools for working with text, like splitting it into words, figuring out what type of word it is, and understanding its meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK-xKFCuj4ac"
      },
      "outputs": [],
      "source": [
        "corpus = \"\"\"Hello, My name is Kamesh Dubey;\n",
        "I am studying Master's of Statistics.\n",
        "My Favourite topic in ML: Semi-Supervised Learning! I recently did a project on it\n",
        "\"\"\"   # This paragraph is called corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "U9f-Dt0p4aVA",
        "outputId": "595d6d42-ddf5-41c6-d2b0-4af2f028dad3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello, My name is Kamesh Dubey;\\nI am studying Master's of Statistics.\\nMy Favourite topic in ML: Semi-Supervised Learning! I recently did a project on it\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaI6FITi4eWR",
        "outputId": "760b2222-dac2-46b9-91e1-ad3ad175bfad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, My name is Kamesh Dubey;\n",
            "I am studying Master's of Statistics.\n",
            "My Favourite topic in ML: Semi-Supervised Learning! I recently did a project on it\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLzLYfkc4QGp"
      },
      "source": [
        "<a id='sec-11'></a>\n",
        "#### **1. Sentence Tokenizer**\n",
        "\n",
        "\n",
        "$$\\text{(Corpus -> Document)}$$\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlKgeIQ-6Wp3",
        "outputId": "7cd9e24e-949b-4d8e-a632-dc824349dcea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcK_1e0S68X_"
      },
      "source": [
        "**Note:**\n",
        "- First-time nltk tokenize use or fresh environment requires **punkt** download.\n",
        "- **sent_tokenize** needs **punkt** tokenizer models; NLTK prompts for download if absent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3EPtTz0jxC6"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize # Sentence Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShfbX9PF6Hzx",
        "outputId": "336f08a4-20d1-4ad9-dbec-e69fc1ad516d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Hello, My name is Kamesh Dubey;\\nI am studying Master's of Statistics.\",\n",
              " 'My Favourite topic in ML: Semi-Supervised Learning!',\n",
              " 'I recently did a project on it']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "documents = sent_tokenize(corpus)\n",
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqvRILWmZeC8",
        "outputId": "7ccc2956-426c-4fc4-b49b-9bd5382b0d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, My name is Kamesh Dubey;\n",
            "I am studying Master's of Statistics.\n",
            "\n",
            "My Favourite topic in ML: Semi-Supervised Learning!\n",
            "\n",
            "I recently did a project on it\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for sentence in documents:\n",
        "  print(sentence)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa5XSlw6Z6wt"
      },
      "source": [
        "<a id='sec-12'></a>\n",
        "#### **2.** Word Tokenizer\n",
        "\n",
        "$$\\textbf{documents -> Word or corpus -> Word}$$\n",
        "\n",
        "---\n",
        "Famous word tokenizers in nltk:\n",
        "\n",
        "2.1 [Word Tokenizer](#sec-21)\n",
        "\n",
        "2.2 [Wordpunct Tokenizer](#sec-22)\n",
        "\n",
        "2.3 [Tree Bank Tokenizer](#sec-23)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj6sNjNXb7BR"
      },
      "source": [
        "<br>\n",
        "\n",
        "<a id='sec-21'></a>\n",
        "**2.1 Word Tokenizer:**\n",
        "\n",
        "It helps break down text into individual words, which is  useful for understanding and analyzing language in various ways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNWZtB8kaFrC"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize # Word Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRgWLD2Jax3F",
        "outputId": "41f246f1-5c92-4c43-c677-b437cbbc4087"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " ',',\n",
              " 'My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Kamesh',\n",
              " 'Dubey',\n",
              " ';',\n",
              " 'I',\n",
              " 'am',\n",
              " 'studying',\n",
              " 'Master',\n",
              " \"'s\",\n",
              " 'of',\n",
              " 'Statistics',\n",
              " '.',\n",
              " 'My',\n",
              " 'Favourite',\n",
              " 'topic',\n",
              " 'in',\n",
              " 'ML',\n",
              " ':',\n",
              " 'Semi-Supervised',\n",
              " 'Learning',\n",
              " '!',\n",
              " 'I',\n",
              " 'recently',\n",
              " 'did',\n",
              " 'a',\n",
              " 'project',\n",
              " 'on',\n",
              " 'it']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "words = word_tokenize(corpus)\n",
        "words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy98VtnCbBRW",
        "outputId": "5497d042-3abd-4429-916d-fca1fa48cf09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'My', 'name', 'is', 'Kamesh', 'Dubey', ';', 'I', 'am', 'studying', 'Master', \"'s\", 'of', 'Statistics', '.']\n",
            "['My', 'Favourite', 'topic', 'in', 'ML', ':', 'Semi-Supervised', 'Learning', '!']\n",
            "['I', 'recently', 'did', 'a', 'project', 'on', 'it']\n"
          ]
        }
      ],
      "source": [
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY86vvK2dzM3"
      },
      "source": [
        "<br>\n",
        "\n",
        "<a id='sec-22'></a>\n",
        "**2.2 WordPunct Tokenizer:**\n",
        "\n",
        "'wordpunct_tokenize' tokenizer splits text into words and punctuation marks, treating punctuation marks also as separate tokens.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "***Note:*** *Word tokenizer splits text into words, while wordpunct_tokenizer additionally tokenizes punctuation marks as separate tokens.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0J7KsZqbZ88",
        "outputId": "bb723df4-6889-4b0b-a09d-b11efc6e7fba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " ',',\n",
              " 'My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Kamesh',\n",
              " 'Dubey',\n",
              " ';',\n",
              " 'I',\n",
              " 'am',\n",
              " 'studying',\n",
              " 'Master',\n",
              " \"'\",\n",
              " 's',\n",
              " 'of',\n",
              " 'Statistics',\n",
              " '.',\n",
              " 'My',\n",
              " 'Favourite',\n",
              " 'topic',\n",
              " 'in',\n",
              " 'ML',\n",
              " ':',\n",
              " 'Semi',\n",
              " '-',\n",
              " 'Supervised',\n",
              " 'Learning',\n",
              " '!',\n",
              " 'I',\n",
              " 'recently',\n",
              " 'did',\n",
              " 'a',\n",
              " 'project',\n",
              " 'on',\n",
              " 'it']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from nltk.tokenize import wordpunct_tokenize #it is also like word tokenizer but it is treating punctuation as seperate words\n",
        "\n",
        "wordpunct_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf38g5i8eMFF"
      },
      "source": [
        "<a id='sec-23'></a>\n",
        "**2.3 TreebankWord Tokenizer**\n",
        "\n",
        "- The TreebankWord Tokenizer in NLTK is good at breaking down text in a way that's commonly seen in English writing.\n",
        "- It knows how to handle contractions like \"don't\" and punctuation. Its tokenization is based on Penn Treebank corpus\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "**For example:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Input Text: \"Don't hesitate to ask questions.\"\n",
        "\n",
        "Tokenized Output: ['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions', '.']\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "In this example, it correctly splits **\"don't\"** into **\"do\"** and **\"n't\"**, and treats punctuation like **\".\"** as separate tokens. This tool is often used for understanding English text in natural language processing.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdsfeiTX0lv_"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize.treebank import TreebankWordTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcnGdUrC0lv_",
        "outputId": "222cb214-fa58-473b-fb77-4feadc1b7855"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " ',',\n",
              " 'My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Kamesh',\n",
              " 'Dubey',\n",
              " ';',\n",
              " 'I',\n",
              " 'am',\n",
              " 'studying',\n",
              " 'Master',\n",
              " \"'s\",\n",
              " 'of',\n",
              " 'Statistics.',\n",
              " 'My',\n",
              " 'Favourite',\n",
              " 'topic',\n",
              " 'in',\n",
              " 'ML',\n",
              " ':',\n",
              " 'Semi-Supervised',\n",
              " 'Learning',\n",
              " '!',\n",
              " 'I',\n",
              " 'recently',\n",
              " 'did',\n",
              " 'a',\n",
              " 'project',\n",
              " 'on',\n",
              " 'it']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "tokenizer.tokenize(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2-LwWbqgc_Y"
      },
      "source": [
        "<a id='sec-2'></a>\n",
        "## **Stemming**\n",
        "\n",
        "$$\\text{Stemming is the process of reducing words to their base or root form to normalize text for analysis.}$$\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "eg:\n",
        "\n",
        "- [going, gone, goes] -> go\n",
        "- [eating, eaten, eats] -> eat\n",
        "<br>\n",
        "1. [Porterstemmer](#sec-31)\n",
        "2. [Snowball Stemmer](#sec-32)\n",
        "3. [RegexpStemmer](#sec-33)\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3RnQR_MkrpU"
      },
      "source": [
        "<a id='sec-31'></a>\n",
        "#### **1. Porterstemmer**\n",
        "\n",
        "---\n",
        "<br>\n",
        "$$\\text{The Porter Stemming algorithm, also known as the PorterStemmer, serves the purpose of removing suffixes from English words to extract their stems.}$$\n",
        "<br>\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://vijinimallawaarachchi.com/wp-content/uploads/2017/05/porterstemmer.png\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <em>Source: <a href=\"https://vijinimallawaarachchi.com/wp-content/uploads/2017/05/porterstemmer.png?w=772\">vijinimallawaarachchi.com</a></em>\n",
        "</p>\n",
        "\n",
        "**Note1**:  \n",
        "\n",
        "- We first find tokenize the **document**.\n",
        "- Then, apply predefined rules of porter stemming algorithm (1a-1e) to accurately strip common suffixes in order to find the stem of the word.\n",
        "\n",
        "**Note2:**\n",
        "- It gives incorrect output for some words\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "$$\\text{Disadvantage of Porter Stemming}$$\n",
        "\n",
        "---\n",
        "1. **Over-stemming:** Porter stemming can excessively strip suffixes, leading to stems that aren't linguistically valid, termed over-stemming.\n",
        "2. **Under-stemming:** Conversely, it may fail to strip all suffixes when necessary, resulting in related words not sharing the same stem, known as under-stemming.\n",
        "3. **Language Specificity:** While effective for English, Porter stemming's rules may not apply well to other languages, limiting its use in multilingual settings.\n",
        "4. **Lack of Semantic Understanding:** Operating purely on string manipulation, the algorithm may miss nuances in word variations due to a lack of semantic comprehension.\n",
        "5. **Performance Trade-offs:** Porter stemming prioritizes efficiency over linguistic precision, necessitating careful consideration of trade-offs between accuracy and speed for specific tasks.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb8cfX3yg0x2"
      },
      "outputs": [],
      "source": [
        "words = [\"going\", \"gone\", \"goes\", \"eating\", \"eaten\", \"eats\", \"finally\", \"finals\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQq_kO5Zi-s6"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW_Hu_DnjDsW"
      },
      "outputs": [],
      "source": [
        "stemming = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "100aLF1CjQJN",
        "outputId": "9210f68e-9083-4daa-b725-6b332e8bcd3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "going------------> go\n",
            "gone------------> gone\n",
            "goes------------> goe\n",
            "eating------------> eat\n",
            "eaten------------> eaten\n",
            "eats------------> eat\n",
            "finally------------> final\n",
            "finals------------> final\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word+\"------------> \"+stemming.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dFRx2r-Yjpvw",
        "outputId": "c591cc55-b684-47a0-8561-3f340f6b1adb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'institut'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "stemming.stem(\"Institute\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uJkoyjZdqU5"
      },
      "source": [
        "<a id='sec-32'></a>\n",
        "#### **2. Snowball Stemmer**\n",
        "---\n",
        "$$\\text{It is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer.}$$\n",
        "<br>\n",
        "<br>\n",
        "**Note:** Snowball supports ***multiple languages*** and not only english.\n",
        "<br>\n",
        "<br>\n",
        "$$\\text{Improvement over Porter Stemmer}$$\n",
        "\n",
        "---\n",
        "\n",
        "- The Snowball Stemmer tends to be **more aggressive** in its stemming approach compared to the Porter Stemmer.\n",
        "- Snowball Stemmer addresses some known issues present in the Porter Stemmer, offering improvements and fixes.\n",
        "- In **Snowball Stemme**r, words like 'fairly' and 'sportingly' are stemmed to 'fair' and 'sport', whereas in **Porter Stemmer**, they are stemmed to 'fairli' and 'sportingli'.\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCe6wNEKb391"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdd63KIxeCk7"
      },
      "outputs": [],
      "source": [
        "snowball_stemmer = SnowballStemmer(\"english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5WCtRjOeQTN",
        "outputId": "87d7fd4f-ec75-4c32-8344-e9d3821fbff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "going----------->go\n",
            "gone----------->gone\n",
            "goes----------->goe\n",
            "eating----------->eat\n",
            "eaten----------->eaten\n",
            "eats----------->eat\n",
            "finally----------->final\n",
            "finals----------->final\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word+\"----------->\"+snowball_stemmer.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR_x0-Yukzn5"
      },
      "source": [
        "<a id='sec-33'></a>\n",
        "#### **3. RegexpStemmer**\n",
        "---\n",
        "$$\\text{A stemmer that uses regular expressions to identify morphological affixes. Any substrings that match the regular expressions will be removed.}$$\n",
        "<br>\n",
        "It combines the power of **regular expressions(re)** with **stemming**. It would use regular expressions to identify patterns in words and then apply stemming rules to reduce those words to their base forms.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "$$\\text{Disadvantage of RegexpStemmer}$$\n",
        "\n",
        "---\n",
        "- RegExpStemmers involve complex regex patterns which can be challenging to handle.\n",
        "- RegExpStemmers lack context awareness in stemming.\n",
        "- They are prone to both overstemming and understemming.\n",
        "- RegExpStemmers can be computationally inefficient, especially with large datasets.\n",
        "- They might not generalize well across different languages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qILYL9SKkzIX"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import RegexpStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaPxxp_mkj7O"
      },
      "outputs": [],
      "source": [
        "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Xze5Ew_uZ2M7",
        "outputId": "8fb48fa1-fa1a-4240-c805-5135f93d28fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'seat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "reg_stemmer.stem(\"seating\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iEbYD8fCZ9QO",
        "outputId": "d12d5473-0fe2-41bb-f2cb-fe41f354fb72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'breath'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "reg_stemmer.stem(\"breathable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lcHCy3WXaFaw",
        "outputId": "7ba5ab53-dfe1-4ef3-d3bf-cab607b9961b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ingseat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "reg_stemmer.stem(\"ingseating\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLA9Z4_4gINP"
      },
      "source": [
        "<a id='sec-3'></a>\n",
        "##  Lemmatization\n",
        "\n",
        "**Wordnet Lemmatizer**\n",
        "\n",
        "---\n",
        "$$\\text{The output we get after lemmatization is called 'lemma', which is a root word rather than root stem.}$$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**How Lemmatization differs from stemming?**\n",
        "- Stemming takes a word down to its root form by **removing its prefixes and\n",
        "suffixes**.\n",
        "- Lemmatization **considers the context and meaning** of a word and tries to convert it to a more meaningful and easier-to-work format.\n",
        "\n",
        "<br>\n",
        "\n",
        "**For example:**\n",
        "- The words **was**, **is**, and **will be** can all be lemmatized to the word **be**.\n",
        "- Similarly, the words **better** and **best** can be lemmatized to the word **good**.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:** Generally, lemmatization is **more sophisticated and accurate** than stemming but can also be more **computationally expensive**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_SrCYR7iFJ_",
        "outputId": "c4a024d8-7430-42a6-fff4-35c74e9d288e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM-irj00iSeK"
      },
      "source": [
        "**Note:**\n",
        "- First-time NLTK use or fresh environment requires **wordnet** download.\n",
        "- **WordNetLemmatizer** needs **wordnet** model; NLTK prompts for download if absent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAhVn_rageHj"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1CWzNlBhxMQ"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h3bmIOcsh3Ht",
        "outputId": "a4d5942c-cee9-4105-da9c-54f6b6838584"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eating'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "lemmatizer.lemmatize(\"eating\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqXzrsyYi2tH"
      },
      "source": [
        "It is giving you result with respect to noun by default, but since POS for **eating** is **verb** we need to pass it manually to lemmatize.\n",
        "\n",
        "**pos**\n",
        "- noun - n\n",
        "- verb = v\n",
        "- adjective - a\n",
        "- adverb - r\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RJM0rlLjh6ku",
        "outputId": "19e957d0-0163-4416-cb63-341ecdeceb64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "lemmatizer.lemmatize(\"eating\", pos='v')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ekGLOXK2j_1c",
        "outputId": "f1d609c1-279f-4bb4-8947-416d40ba635d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'strong'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "lemmatizer.lemmatize(\"strongest\", pos='a')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kTfK-tzk-uo"
      },
      "source": [
        "<a id='sec-4'></a>\n",
        "# **Stopwords**\n",
        "---\n",
        "$$\\text{Stopwords are the words in any language which does not add much meaning to a sentence and can be removed without sacrificing the meaning of sentence.}$$\n",
        "<br>\n",
        "**Note1:** \"stop words\" usually refers to the most common words in a language.\n",
        "\n",
        "**Note2:** stopwords from different language is available in nltk library.\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "$$\\textbf{Pros and Cons of Stopwords}$$\n",
        "\n",
        "---\n",
        "**Pros:**\n",
        "- ***Efficiency:*** Removing stop words reduces dataset size and training time.\n",
        "- ***Improved Performance:*** Eliminating stop words enhances token significance and classification accuracy.\n",
        "\n",
        "**Cons:**\n",
        "- ***Semantic Alteration:*** Improper stop word selection can change text meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4PL2SMZlBla"
      },
      "outputs": [],
      "source": [
        "corpus = \"\"\"Semi-supervised learning sits between two types of machine learning. In regular supervised learning, we only train models with labeled data. In\n",
        "unsupervised learning, models explore unlabeled data. Semi-supervised learning cleverly uses both labeled and unlabeled data to make models better. Imagine\n",
        "you have some photos of cats, but not all of them are labeled \"cat.\" Semi-supervised learning helps by using both the labeled \"cat\" photos and the unlabeled\n",
        "ones to improve its understanding of what a cat looks like.One trick it uses is to look at all the photos, labeled or not, and find  similarities between them.\n",
        "This helps the model learn better, especially when there aren't many labeled photos to learn from.Another way it works is by making sure the model gives similar\n",
        "answers for similar-looking photos, whether they're labeled or not. This makes the model more reliable and better at figuring out new, unseen photos. Semi-\n",
        "supervised learning is handy in lots of areas, like recognizing objects in pictures, understanding language, or even understanding spoken words. It's especially\n",
        "useful when there aren't many labeled examples to learn from but plenty of unlabeled data around.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW6FylGTQP-w",
        "outputId": "79729cc3-c833-4736-dc45-26b0fd133aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icfIZ7etJxKp"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs01MsAGJ0_N",
        "outputId": "ae88f6db-dc44-4db0-ee4a-67bb5798407a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "stopwords.words(\"english\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcVEGp3nQucu"
      },
      "source": [
        "**Note:** $$\\textbf{Often you will need to do modification i.e. add or remove stopword suitable to your use case}$$\n",
        "$$\\text{For example: You my sometime want to 'not' outside the stopwords to for negative sentiments.}$$\n",
        "\n",
        "Let's use it in corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVjTBD-pVwPx"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WW1Jz71KD3h",
        "outputId": "64a3e3fd-e0b7-4f1b-b6b5-06aabc847280"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Semi-supervised learning sits between two types of machine learning.',\n",
              " 'In regular supervised learning, we only train models with labeled data.',\n",
              " 'In\\nunsupervised learning, models explore unlabeled data.',\n",
              " 'Semi-supervised learning cleverly uses both labeled and unlabeled data to make models better.',\n",
              " 'Imagine\\nyou have some photos of cats, but not all of them are labeled \"cat.\"',\n",
              " 'Semi-supervised learning helps by using both the labeled \"cat\" photos and the unlabeled\\nones to improve its understanding of what a cat looks like.One trick it uses is to look at all the photos, labeled or not, and find  similarities between them.',\n",
              " \"This helps the model learn better, especially when there aren't many labeled photos to learn from.Another way it works is by making sure the model gives similar\\nanswers for similar-looking photos, whether they're labeled or not.\",\n",
              " 'This makes the model more reliable and better at figuring out new, unseen photos.',\n",
              " 'Semi-\\nsupervised learning is handy in lots of areas, like recognizing objects in pictures, understanding language, or even understanding spoken words.',\n",
              " \"It's especially\\nuseful when there aren't many labeled examples to learn from but plenty of unlabeled data around.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "nltk.sent_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii5Db3V2SfbV",
        "outputId": "75563560-f2c9-470a-d15f-4356748b4343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk.sent_tokenize(corpus)[0] gives : Semi-supervised learning sits between two types of machine learning. \n",
            "\n",
            "semi-supervis\n",
            "learn\n",
            "sit\n",
            "two\n",
            "type\n",
            "machin\n",
            "learn\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "# Applying Stemming on first tokenized sentence\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "print(\"nltk.sent_tokenize(corpus)[0] gives :\", nltk.sent_tokenize(corpus)[0], \"\\n\") # printing the fist sentences of corpus after sentence tokenization.\n",
        "\n",
        "for word in nltk.word_tokenize(nltk.sent_tokenize(corpus)[0]): # We word tokenize the first document from corpus and loop over it\n",
        "  if word not in stopwords.words(\"english\"):  # if word is not a stopword\n",
        "    print(stemmer.stem(word)) # gets it's stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6_cJgyaSgFY",
        "outputId": "2e016dba-ca85-4fd0-c017-63e46d91a3cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['semi-supervis learn sit two type machin learn .',\n",
              " 'in regular supervis learn , train model label data .',\n",
              " 'in unsupervis learn , model explor unlabel data .',\n",
              " 'semi-supervis learn cleverli use label unlabel data make model better .',\n",
              " \"imagin photo cat , label `` cat . ''\",\n",
              " \"semi-supervis learn help use label `` cat '' photo unlabel one improv understand cat look like.on trick use look photo , label , find similar .\",\n",
              " \"thi help model learn better , especi n't mani label photo learn from.anoth way work make sure model give similar answer similar-look photo , whether 're label .\",\n",
              " 'thi make model reliabl better figur new , unseen photo .',\n",
              " 'semi- supervis learn handi lot area , like recogn object pictur , understand languag , even understand spoken word .',\n",
              " \"it 's especi use n't mani label exampl learn plenti unlabel data around .\"]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "documents = sent_tokenize(corpus)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "for idx in range(len(documents)): # We word tokenize the first document from corpus and loop over it\n",
        "  words = nltk.word_tokenize(documents[idx])\n",
        "  words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))] # if word is not a stopword get its stem\n",
        "  documents[idx] = ' '.join(words)\n",
        "\n",
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ng599ZWWR7w",
        "outputId": "9580354b-6458-4f0c-97f5-fa632cbffe08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['semi-supervis learn sit two type machin learn .',\n",
              " 'in regular supervis learn , train model label data .',\n",
              " 'in unsupervis learn , model explor unlabel data .',\n",
              " 'semi-supervis learn clever use label unlabel data make model better .',\n",
              " \"imagin photo cat , label `` cat . ''\",\n",
              " \"semi-supervis learn help use label `` cat '' photo unlabel one improv understand cat look like.on trick use look photo , label , find similar .\",\n",
              " \"this help model learn better , especi n't mani label photo learn from.anoth way work make sure model give similar answer similar-look photo , whether re label .\",\n",
              " 'this make model reliabl better figur new , unseen photo .',\n",
              " 'semi- supervis learn handi lot area , like recogn object pictur , understand languag , even understand spoken word .',\n",
              " \"it 's especi use n't mani label exampl learn plenti unlabel data around .\"]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "documents = sent_tokenize(corpus)\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "for idx in range(len(documents)): # We word tokenize the first document from corpus and loop over it\n",
        "  words = nltk.word_tokenize(documents[idx])\n",
        "  words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))] # if word is not a stopword get its stem\n",
        "  documents[idx] = ' '.join(words)\n",
        "\n",
        "documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX0Pq3frWkgK"
      },
      "source": [
        "**Note:** In Snowball, all words are in lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09LbOyBYWrqA",
        "outputId": "fe1a0167-8d5a-4f2b-8f2c-0e9784918196"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['semi-supervised learn sit two type machine learn .',\n",
              " 'in regular supervise learn , train model label data .',\n",
              " 'in unsupervised learn , model explore unlabeled data .',\n",
              " 'semi-supervised learn cleverly use label unlabeled data make model better .',\n",
              " \"imagine photos cat , label `` cat . ''\",\n",
              " \"semi-supervised learn help use label `` cat '' photos unlabeled ones improve understand cat look like.one trick use look photos , label , find similarities .\",\n",
              " \"this help model learn better , especially n't many label photos learn from.another way work make sure model give similar answer similar-looking photos , whether 're label .\",\n",
              " 'this make model reliable better figure new , unseen photos .',\n",
              " 'semi- supervise learn handy lot areas , like recognize object picture , understand language , even understand speak word .',\n",
              " \"it 's especially useful n't many label examples learn plenty unlabeled data around .\"]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "documents = sent_tokenize(corpus)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for idx in range(len(documents)): # We word tokenize the first document from corpus and loop over it\n",
        "  words = nltk.word_tokenize(documents[idx])\n",
        "  words = [lemmatizer.lemmatize(word, pos='v').lower() for word in words if word not in set(stopwords.words('english'))] # if word is not a stopword lemmitize it and then lowercase\n",
        "  documents[idx] = ' '.join(words)\n",
        "\n",
        "documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxwPAeNXLTcb"
      },
      "source": [
        "<a id='sec-5'></a>\n",
        "# **Part of Speech Tagging**\n",
        "---\n",
        "$$\\text{It involves assigning a part-of-speech tag (such as noun, verb, adjective, etc.) to each word in a given text.}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "<div style=\"text-align:center;\">\n",
        "    <img src=\"https://byteiota.com/wp-content/uploads/2021/01/POS-Tagging-800x400.jpg\" alt=\"POS Tagging Image\" style=\"width:700px;height:250px;\"/>\n",
        "</div>\n",
        "\n",
        "<p style=\"text-align:center;\"><em>Source: <a href=\"https://byteiota.com/wp-content/uploads/2021/01/POS-Tagging-800x400.jpg\">ByteIota</a></em></p>\n",
        "\n",
        "<br>\n",
        "\n",
        "**The purpose of POS tagging is:**\n",
        "-  to analyze the grammatical structure of sentences\n",
        "-  identify the syntactic roles of individual words within sentences.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:** POS tagging can be performed using various techniques, including rule-based approaches, statistical models, and deep learning methods.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Some important encoding we get will nltk pos-tagging with what each represesnt.**\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "| Tag  | Description                               | Example           |\n",
        "|------|-------------------------------------------|-------------------|\n",
        "| NN   | Noun, singular or mass                    | **cat**, **dog**, **book** |\n",
        "| NNS  | Noun, plural                              | **cats**, **dogs**, **books** |\n",
        "| NNP  | Proper noun, singular                     | **John**, **London**, **Monday** |\n",
        "| NNPS | Proper noun, plural                       | **Smiths**, **Americans**, **Androids** |\n",
        "| VB   | Verb, base form                           | **run**, **walk**, **jump** |\n",
        "| VBD  | Verb, past tense                          | **ran**, **walked**, **jumped** |\n",
        "| VBG  | Verb, gerund or present participle        | **running**, **walking**, **jumping** |\n",
        "| VBN  | Verb, past participle                     | **run**, **walked**, **jumped** |\n",
        "| VBP  | Verb, non-3rd person singular present     | **am**, **are**, **have** |\n",
        "| VBZ  | Verb, 3rd person singular present         | **is**, **has**, **does** |\n",
        "| JJ   | Adjective                                 | **big**, **red**, **tall** |\n",
        "| JJR  | Adjective, comparative                    | **bigger**, **redder**, **taller** |\n",
        "| JJS  | Adjective, superlative                    | **biggest**, **reddest**, **tallest** |\n",
        "| RB   | Adverb                                    | **quickly**, **happily**, **loudly** |\n",
        "| RBR  | Adverb, comparative                       | **faster**, **happier**, **louder** |\n",
        "| RBS  | Adverb, superlative                       | **fastest**, **happiest**, **loudest** |\n",
        "| IN   | Preposition or subordinating conjunction  | **in**, **on**, **at** |\n",
        "| PRP  | Personal pronoun                          | **I**, **you**, **he** |\n",
        "| PRP$ | Possessive pronoun                        | **my**, **your**, **his** |\n",
        "| DT   | Determiner                                | **the**, **a**, **an** |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoiQlO1_LVzO"
      },
      "outputs": [],
      "source": [
        "sentence = \"I live in most beautiful mumbai city\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkTGR2yKLWu7",
        "outputId": "6b279140-b7e8-4783-8a19-140875deb318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Sfttb230lwH"
      },
      "source": [
        "**Note:**\n",
        "- First-time NLTK use or fresh environment requires you to download tagger\n",
        "- NLTK prompts for download if absent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xElP0XuHXslk"
      },
      "source": [
        "**Note:**\n",
        "- First-time nltk pos_tagger use or fresh environment requires **punkt** download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4ok8xHPLWrc"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmzVeNJSLWoZ",
        "outputId": "792f3a99-3e5f-470f-bd90-81774cc7b4a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'live', 'in', 'most', 'beautiful', 'mumbai', 'city']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "sen_token = word_tokenize(sentence)\n",
        "sen_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoXv0aUBLWli",
        "outputId": "efef10b8-abd7-47ad-cdc1-d321134d796d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('live', 'VBP'),\n",
              " ('in', 'IN'),\n",
              " ('most', 'JJS'),\n",
              " ('beautiful', 'JJ'),\n",
              " ('mumbai', 'NN'),\n",
              " ('city', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "pos_tag(sen_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5mLPje1X1o2"
      },
      "source": [
        "- pos_tagging on tokenized words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9Fhw3-bLWiu",
        "outputId": "d9a8f107-dc48-49f9-8757-b3ddac47eb86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('live', 'VBP'),\n",
              " ('in', 'IN'),\n",
              " ('most', 'JJS'),\n",
              " ('beautiful', 'JJ'),\n",
              " ('mumbai', 'NN'),\n",
              " ('city', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "pos_tag(sentence.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufx8JiurX8eV"
      },
      "source": [
        "- pos_tagging on under unprocessed words in sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEJpoEdNLXP2"
      },
      "source": [
        "<a id='sec-6'></a>\n",
        "# **Name-Entity Recoginition**\n",
        "---\n",
        "<br>\n",
        "$$\\text{\n",
        "Named Entity Recognition (NER) detects and categorizes specific entities like names, organizations, and locations in unstructured text.\n",
        "}$$\n",
        "<br>\n",
        "<div style=\"text-align:center;\">\n",
        "    <img src=\"https://monkeylearn.com/static/d0575562cdedb47340c00662c5c1b859/63bf9/Example.png\" alt=\"Example Image\" style=\"width:500px;\"/>\n",
        "</div>\n",
        "   \n",
        "<p style=\"text-align:center;\"><em>Source: <a href=\"https://monkeylearn.com/static/d0575562cdedb47340c00662c5c1b859/63bf9/Example.png\">MonkeyLearn</a></em></p>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Note 1:** NER enables the extraction of structured data from documents, improving search, analytics, and integration across systems.\n",
        "\n",
        "**Note 2:** NER can be performed using various techniques, including rule-based approaches, ML method like HMM and deep learning methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KvcZXXlSoln",
        "outputId": "dfdf3bbd-0393-4e96-ae65-a6df9570da33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl88NcZw0lwI"
      },
      "source": [
        "**Note:**\n",
        "- First-time NLTK use or fresh environment requires **maxent_ne_chunker** and **word** download for nltk NER.\n",
        "- NLTK prompts for download if absent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8Bmmn5HTqIB"
      },
      "source": [
        "$$\\textbf{You may need to install:}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDuFhW8DTo0e",
        "outputId": "35c1981c-7524-4663-a881-3448c365e054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting svgling\n",
            "  Downloading svgling-0.4.0-py3-none-any.whl (23 kB)\n",
            "Collecting svgwrite (from svgling)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/67.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m61.4/67.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.4.0 svgwrite-1.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install svgling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaWm8vbUT1jz"
      },
      "source": [
        "**Now, let's do NER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGE1GchQL--e"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NkYweRKLclA"
      },
      "outputs": [],
      "source": [
        "sentence = \"Ousted WeWork founder Adam Neumann lists his Manhattan penthouse for $37.5 million\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdSjfEClL-7G"
      },
      "outputs": [],
      "source": [
        "pos_tagged = pos_tag(nltk.word_tokenize(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "7aS4d2YGL-4A",
        "outputId": "70a7bb28-ad74-4274-d6fa-49502c6acfb6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [('Ousted', 'VBN'), Tree('ORGANIZATION', [('WeWork', 'NNP')]), ('founder', 'NN'), Tree('PERSON', [('Adam', 'NNP'), ('Neumann', 'NNP')]), ('lists', 'VBZ'), ('his', 'PRP$'), Tree('GPE', [('Manhattan', 'NNP')]), ('penthouse', 'NN'), ('for', 'IN'), ('$', '$'), ('37.5', 'CD'), ('million', 'CD')])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,832.0,168.0\" width=\"832px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"7.69231%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Ousted</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.84615%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"13.4615%\" x=\"7.69231%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">WeWork</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.4231%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"8.65385%\" x=\"21.1538%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">founder</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25.4808%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"14.4231%\" x=\"29.8077%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"40%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Adam</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"60%\" x=\"40%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Neumann</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.0192%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.73077%\" x=\"44.2308%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">lists</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.5962%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.76923%\" x=\"50.9615%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">his</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.8462%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"10.5769%\" x=\"56.7308%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Manhattan</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.0192%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"10.5769%\" x=\"67.3077%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">penthouse</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.5962%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.80769%\" x=\"77.8846%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">for</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80.2885%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.88462%\" x=\"82.6923%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">$</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"84.1346%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.76923%\" x=\"85.5769%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">37.5</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.4615%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"8.65385%\" x=\"91.3462%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">million</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"95.6731%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "nltk.ne_chunk(pos_tagged)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}