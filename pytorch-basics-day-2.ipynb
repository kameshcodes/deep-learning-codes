{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNj+23IhvIIvYoGFWXIVwRs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameshcodes/deep-learning-codes/blob/main/pytorch_basics_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\textbf{On Day 1, We Learned}$$\n",
        "\n",
        "**[Day 1 Notebook](https://github.com/kameshcodes/deep-learning-codes/blob/main/pytorch_basics.ipynb)**\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "nmiNs-NXqf9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $\\textbf{1.Tensor Basics}$\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### $\\textbf{But Why Tensors and not arrays or dataframes ?}$\n",
        "\n",
        "\n",
        "While NumPy arrays and Pandas dataframes has been useful for numerical computations and data manipulation, PyTorch tensors offer several advantages for machine learning, especially in deep learning while training neural networks:\n",
        "\n",
        "#### GPU Acceleration:\n",
        "- **Tensors**: PyTorch tensors can be easily transferred to GPUs, enabling faster computations crucial for training large neural networks.\n",
        "- **NumPy Arrays/Pandas DataFrames**: Primarily designed for CPU operations. GPU support via libraries like CuPy is less seamless than in PyTorch.\n",
        "\n",
        "#### - Automatic Differentiation:\n",
        "- **Tensors**: PyTorch's `autograd` package works with tensors to automatically compute gradients, essential for backpropagation in neural network training.\n",
        "- **NumPy Arrays/Pandas DataFrames**: Do not inherently support automatic differentiation, making manual gradient calculations cumbersome and error-prone.\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "rQTx0xfmugTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import random\n",
        "random.seed(1) #for reprodicibility"
      ],
      "metadata": {
        "id": "B3-XcANeu-WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{1.1 Tensor Creation}$"
      ],
      "metadata": {
        "id": "b77RTJqH3HKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{1.2 Tensor Operations}$"
      ],
      "metadata": {
        "id": "QYOrJuIn3lQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{1.3 Slicing in Tensors}$"
      ],
      "metadata": {
        "id": "vEZeR8Vg8g0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{1.4 Resize a Tensor}$"
      ],
      "metadata": {
        "id": "kHVh_q4R_w0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{1.5 Numpy <=> Tensor}$"
      ],
      "metadata": {
        "id": "-3tMD3BYCnRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{1.6 Creating Tensors on CUDA GPU}$"
      ],
      "metadata": {
        "id": "08WPL6plErtL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $\\textbf{2. AutoGrad in Pytorch}$\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "PyTorch's $\\textbf{autograd}$ it is a tool that automatically calculates the gradients needed in the backpropagation step of learning models, particularly neural networks.\n",
        "\n",
        "- It works by creating a $\\textbf{dynamic computational graph}$ as you perform operations, which makes it very flexible and easy to debug.\n",
        "\n",
        "- To use autograd, you simply set **`requires_grad=True`** on the tensors you want to track. When you perform operations on these tensors, PyTorch keeps track of them. Later, you can call the $\\textbf{backward()}$ method on the final result to compute the gradients, which will be stored in the $\\textbf{.grad}$ attribute of the original tensors.\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\text{What is a computational graph?}$\n",
        "\n",
        " A computational graph is a dynamic representation of the operations (like addition, multiplication, etc.)  performed on tensors during the forward pass of a neural network.\n",
        "- a computational graph is typically represented as a $\\text{Directed Acyclic Graph (DAG)}$\n",
        "- In PyTorch, computational graphs are dynamic, meaning they are constructed dynamically as operations are performed during the execution of the forward pass.\n"
      ],
      "metadata": {
        "id": "FJOccEIBaQze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{2.1 Intialize Autograd for a tensor}$"
      ],
      "metadata": {
        "id": "mIaGPNIqy7i8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{2.2 Visualizing Computation Graph in PyTorch}$"
      ],
      "metadata": {
        "id": "F_EYGLSnjwf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{2.3 Gradient Calculation}$"
      ],
      "metadata": {
        "id": "mknFboKszPoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Day 2**\n",
        "\n",
        "$$\\textbf{Pytorch Day 2}$$\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "G74nqH4k0OFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $\\textbf{3. BackPropagation}$\n",
        "\n",
        "$\\text{For every operation we do with tensors, Pytorch create a computational graph for us.}$\n",
        "\n",
        "$\\text{Suppose we have two tensors x and y:}$\n",
        "- $\\text{Then at perform an operation f(.) on tensors x and y to get a node representing z=f(x,y)}$\n",
        "- $\\text{Now at these nodes z=f(x,y), we do calculate local gradients which we can use later in chain rule for calculation of final gradient.}$\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>\n",
        "\n",
        "![Img](https://polakowo.io/datadocs/assets/1*q1M7LGiDTirwU-4LcFq7_Q.png)\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "$\\text{Whole Concept Consist of 3 Steps:}$\n",
        "\n",
        "1. **Forward Pass:** Compute $Loss$\n",
        "2. **Compute local gradients**\n",
        "3. **Backward Pass:**  Compute $\\frac{d(Loss)}{d\\text{weights}}$ using the chain rule.\n"
      ],
      "metadata": {
        "id": "iktjohjH4vaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{3.1 Backprop in Pytorch}$\n",
        "---"
      ],
      "metadata": {
        "id": "7XBuzmr5FOme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "s3cmolkq0iCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets say we have data x, y\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0) # not we dont turn on auto grade here coz we need to update only weights and not change data\n",
        "\n",
        "# initialize a random weight\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "# Step 1: Forward Pass\n",
        "y_hat = w*x\n",
        "loss = (y_hat - y)**2\n",
        "\n",
        "# Step 2: Pytorch will calculate gradient automatically since autograd is on\n",
        "# Step 3: backward pass\n",
        "loss.backward()\n",
        "print(w.grad)"
      ],
      "metadata": {
        "id": "ieO3_Ddy9wwb",
        "outputId": "36bb9676-e2bc-4652-b4e7-048dd726200e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{We then update weights and do couple of epochs of forward and backward pass to get optimum weights.}$\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "dMYUiM-S_LnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\textbf{3.2 Gradient Descent in Linear Regression}$\n",
        "---"
      ],
      "metadata": {
        "id": "LEAFckWh_aop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\text{3.2.1 SLR implementation With Numpy}$\n",
        "\n",
        "Lets implement simple linear regression from scratch using $Numpy$"
      ],
      "metadata": {
        "id": "hhGYaenmEYPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "BL8cbvVd_Kyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "def loss(y, y_pred):\n",
        "  return ((y-y_pred)**2).mean()\n",
        "\n",
        "#gradient\n",
        "#MSE, J= (1/N) * (w*x - y)**2\n",
        "#Gradient dJ/dx = (1/N) * 2 * x * (w*x - y)\n",
        "\n",
        "def gradient(x,y, y_pred):\n",
        "  return np.dot(2*x, y_pred-y).mean()"
      ],
      "metadata": {
        "id": "PRqfpCvABv6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets assume f = 2*x : this is true objective function\n",
        "X = np.array([1,2,3,4], dtype=np.float32)\n",
        "y = np.array(2*X, dtype=np.float32)\n",
        "\n",
        "\n",
        "w = 0.0  #Initializing weights as zero\n",
        "lr = 0.005\n",
        "epochs = 20\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nPrediction before training: f(5) = {forward(5):.3f}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  y_pred = forward(X)\n",
        "  l = loss(y, y_pred)\n",
        "  dw = gradient(X, y, y_pred)\n",
        "  w = w - lr*dw\n",
        "  if epoch%1 == 0:\n",
        "    print(f\"epoch {epoch+1}: w = {w:.3f} and loss = {l:.5f}\")\n",
        "\n",
        "print(f\"\\nPrediction after training: f(5) = {forward(5):.3f}\\n\")"
      ],
      "metadata": {
        "id": "SeoIPiAvB27c",
        "outputId": "267ba2f6-ab14-41b7-9e3a-d29bb0022327",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction before training: f(5) = 0.000\n",
            "\n",
            "epoch 1: w = 0.600 and loss = 30.00000\n",
            "epoch 2: w = 1.020 and loss = 14.70000\n",
            "epoch 3: w = 1.314 and loss = 7.20300\n",
            "epoch 4: w = 1.520 and loss = 3.52947\n",
            "epoch 5: w = 1.664 and loss = 1.72944\n",
            "epoch 6: w = 1.765 and loss = 0.84743\n",
            "epoch 7: w = 1.835 and loss = 0.41524\n",
            "epoch 8: w = 1.885 and loss = 0.20347\n",
            "epoch 9: w = 1.919 and loss = 0.09970\n",
            "epoch 10: w = 1.944 and loss = 0.04885\n",
            "epoch 11: w = 1.960 and loss = 0.02394\n",
            "epoch 12: w = 1.972 and loss = 0.01173\n",
            "epoch 13: w = 1.981 and loss = 0.00575\n",
            "epoch 14: w = 1.986 and loss = 0.00282\n",
            "epoch 15: w = 1.991 and loss = 0.00138\n",
            "epoch 16: w = 1.993 and loss = 0.00068\n",
            "epoch 17: w = 1.995 and loss = 0.00033\n",
            "epoch 18: w = 1.997 and loss = 0.00016\n",
            "epoch 19: w = 1.998 and loss = 0.00008\n",
            "epoch 20: w = 1.998 and loss = 0.00004\n",
            "\n",
            "Prediction after training: f(5) = 9.992\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\text{3.2.2 SLR implementation With Pytorch}$\n",
        "\n",
        "Lets implement simple linear regression from scratch using Pytorch $Autograd$"
      ],
      "metadata": {
        "id": "lBL2SIHOFat4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "def loss(y, y_pred):\n",
        "  return ((y-y_pred)**2).mean()\n",
        "\n",
        "#gradient\n",
        "#MSE, J= (1/N) * (w*x - y)**2\n",
        "#Gradient dJ/dx = (1/N) * 2 * x * (w*x - y)\n",
        "\n",
        "# def gradient(x,y, y_pred):\n",
        "#   return np.dot(2*x, y_pred-y).mean() #this is not nedded pytorch will calculate it automatically"
      ],
      "metadata": {
        "id": "oDwHzlcuF7T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets assume f = 2*x : this is true objective function\n",
        "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
        "y = 2*X\n",
        "\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)  #Initializing weights as zero\n",
        "\n",
        "lr = 0.005\n",
        "epochs = 20\n",
        "\n",
        "\n",
        "print(f\"\\nPrediction before training: f(5) = {forward(5):.3f}\\n\")\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #foreard pass\n",
        "  y_pred = forward(X)\n",
        "  l = loss(y, y_pred)\n",
        "\n",
        "  #backward pass\n",
        "  l.backward() #dl/dw :calcuate gradient\n",
        "\n",
        "  with torch.no_grad():\n",
        "      w-=(lr*w.grad)\n",
        "\n",
        "  #zero the grad\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if epoch%1 == 0:\n",
        "      print(f\"epoch {epoch+1}: w = {w:.3f} and loss = {l:.5f}\")\n",
        "\n",
        "print(f\"\\nPrediction after training: f(5) = {forward(5):.3f}\\n\")"
      ],
      "metadata": {
        "id": "2GUrnGezqjQv",
        "outputId": "0e965582-fe96-4af0-ccc8-a35bb7a60be2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction before training: f(5) = 0.000\n",
            "\n",
            "epoch 1: w = 0.150 and loss = 30.00000\n",
            "epoch 2: w = 0.289 and loss = 25.66875\n",
            "epoch 3: w = 0.417 and loss = 21.96283\n",
            "epoch 4: w = 0.536 and loss = 18.79194\n",
            "epoch 5: w = 0.646 and loss = 16.07886\n",
            "epoch 6: w = 0.747 and loss = 13.75747\n",
            "epoch 7: w = 0.841 and loss = 11.77124\n",
            "epoch 8: w = 0.928 and loss = 10.07176\n",
            "epoch 9: w = 1.008 and loss = 8.61765\n",
            "epoch 10: w = 1.083 and loss = 7.37348\n",
            "epoch 11: w = 1.152 and loss = 6.30893\n",
            "epoch 12: w = 1.215 and loss = 5.39808\n",
            "epoch 13: w = 1.274 and loss = 4.61873\n",
            "epoch 14: w = 1.329 and loss = 3.95190\n",
            "epoch 15: w = 1.379 and loss = 3.38135\n",
            "epoch 16: w = 1.425 and loss = 2.89317\n",
            "epoch 17: w = 1.469 and loss = 2.47547\n",
            "epoch 18: w = 1.508 and loss = 2.11807\n",
            "epoch 19: w = 1.545 and loss = 1.81227\n",
            "epoch 20: w = 1.579 and loss = 1.55063\n",
            "\n",
            "Prediction after training: f(5) = 7.897\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets assume f = 2*x : this is true objective function\n",
        "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
        "y = 2*X\n",
        "\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)  #Initializing weights as zero\n",
        "\n",
        "lr = 0.005\n",
        "epochs = 20\n",
        "\n",
        "\n",
        "print(f\"\\nPrediction before training: f(5) = {forward(5):.3f}\\n\")\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #foreard pass\n",
        "  y_pred = forward(X)\n",
        "  l = loss(y, y_pred)\n",
        "\n",
        "  #backward pass\n",
        "  l.backward() #dl/dw :calcuate gradient\n",
        "\n",
        "  with torch.no_grad():\n",
        "      w=w-(lr*w.grad)\n",
        "\n",
        "  #zero the grad\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if epoch%1 == 0:\n",
        "      print(f\"epoch {epoch+1}: w = {w:.3f} and loss = {l:.5f}\")\n",
        "\n",
        "print(f\"\\nPrediction after training: f(5) = {forward(5):.3f}\\n\")"
      ],
      "metadata": {
        "id": "J6bSpD7LFkPG",
        "outputId": "ad9a11e0-ac33-45d6-ab1a-00bbc0f86a1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction before training: f(5) = 0.000\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'zero_'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1d162b2043a1>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;31m#zero the grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'zero_'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{Remarks: }$\n",
        "\n",
        "----\n",
        "\n",
        "- In PyTorch, during backpropagation, gradients are calculated symbolically using automatic differentiation. This means that PyTorch keeps a record of all the mathematical operations performed during the forward pass to determine how each parameter affects the loss. However, due to the complexity of symbolic differentiation, small errors can occur in these calculations. As a result, the convergence, or the process of reaching the optimal solution, may be slower compared to methods that perform exact numerical computations.\n",
        "\n",
        "- Always update weights inplace $w-=(lr*w.grad)$ and **not** $w= w-(lr*w.grad)$, later gives error because When you assign a new value to a variable (in this case, the weights $w$) without modifying it in place, it breaks the connection between the variable and the computational graph. This can lead to issues with backpropagation, where the gradients are propagated backward through the graph to update the weights during training.\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "6VDhFGF2edns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\text{3.2.3 Training Pipeline: Model, Loss, and optimizer}$\n",
        "\n",
        "\n",
        "$\\text{General Training Pipeline in pytorch:}$\n",
        "\n",
        "1. Design our model: (input_size, output_size, forward_pass)\n",
        "2. Construct loss and optimizers\n",
        "3. Training Loop\n",
        "\n",
        " - forward pass: compute prediction\n",
        "\n",
        " - backward pass: gradients\n",
        "\n",
        " - update weights"
      ],
      "metadata": {
        "id": "Ox6-gIqhhauw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "BiI9Dmd0haOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input tensor X and target tensor y\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "y = 2 * X\n",
        "\n",
        "# Define a test input tensor X_test\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "# Set learning rate and number of epochs\n",
        "lr = 0.05\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "NP5Vt8kamjkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of samples and features from input tensor X\n",
        "n_samples, n_features = X.shape\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "\n",
        "# Define a simple linear regression model\n",
        "model = nn.Linear(input_size, output_size)"
      ],
      "metadata": {
        "id": "4ogcpytUmZ2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define mean squared error loss function\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# Define stochastic gradient descent optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "n8MOxz1NmeNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass: predict y using the model\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # Calculate the loss\n",
        "    l = loss(y, y_pred)\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    l.backward()\n",
        "\n",
        "    # Update model parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Reset gradients to zero\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Print progress every epoch\n",
        "    if epoch % 1 == 0:\n",
        "        w, b = model.parameters()\n",
        "        print(f\"epoch {epoch + 1}: w = {w[0].item():.3f} and loss = {l:.5f}\")\n",
        "\n",
        "# Print prediction for the test input after training\n",
        "print(f\"\\nPrediction after training: {model(X_test).item():.3f}\\n\")\n"
      ],
      "metadata": {
        "id": "i8W2j5jwedPP",
        "outputId": "359d0b3f-c51d-4ac0-ccf0-35b337a2d02b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1: w = 1.467 and loss = 37.92918\n",
            "epoch 2: w = 1.748 and loss = 1.08841\n",
            "epoch 3: w = 1.797 and loss = 0.08412\n",
            "epoch 4: w = 1.807 and loss = 0.05517\n",
            "epoch 5: w = 1.811 and loss = 0.05281\n",
            "epoch 6: w = 1.814 and loss = 0.05122\n",
            "epoch 7: w = 1.817 and loss = 0.04970\n",
            "epoch 8: w = 1.820 and loss = 0.04822\n",
            "epoch 9: w = 1.823 and loss = 0.04679\n",
            "epoch 10: w = 1.825 and loss = 0.04540\n",
            "epoch 11: w = 1.828 and loss = 0.04405\n",
            "epoch 12: w = 1.831 and loss = 0.04274\n",
            "epoch 13: w = 1.833 and loss = 0.04147\n",
            "epoch 14: w = 1.836 and loss = 0.04024\n",
            "epoch 15: w = 1.838 and loss = 0.03904\n",
            "epoch 16: w = 1.840 and loss = 0.03788\n",
            "epoch 17: w = 1.843 and loss = 0.03676\n",
            "epoch 18: w = 1.845 and loss = 0.03566\n",
            "epoch 19: w = 1.847 and loss = 0.03460\n",
            "epoch 20: w = 1.850 and loss = 0.03358\n",
            "\n",
            "Prediction after training: 9.691\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\text{3.2.4 Custom Linear Regression Model}$"
      ],
      "metadata": {
        "id": "9MIh-mM2mCIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "\n",
        "# data\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "y = 2 * X\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "lr = 0.05\n",
        "epochs = 20\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super(LinearRegression, self).__init__()\n",
        "    #define model layers\n",
        "    self.linear = nn.Linear(input_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define mean squared error loss function\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# Define stochastic gradient descent optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass: predict y using the model\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # Calculate the loss\n",
        "    l = loss(y, y_pred)\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    l.backward()\n",
        "\n",
        "    # Update model parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Reset gradients to zero\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Print progress every epoch\n",
        "    if epoch % 1 == 0:\n",
        "        w, b = model.parameters()\n",
        "        print(f\"epoch {epoch + 1}: w = {w[0].item():.3f} and loss = {l:.5f}\")\n",
        "\n",
        "# Print prediction for the test input after training\n",
        "print(f\"\\nPrediction after training: {model(X_test).item():.3f}\\n\")\n"
      ],
      "metadata": {
        "id": "R-MXZKK_nujJ",
        "outputId": "f9cbf325-d934-4550-92f6-443c49d7a697",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1: w = 1.484 and loss = 7.00944\n",
            "epoch 2: w = 1.607 and loss = 0.38854\n",
            "epoch 3: w = 1.632 and loss = 0.20248\n",
            "epoch 4: w = 1.641 and loss = 0.19171\n",
            "epoch 5: w = 1.646 and loss = 0.18588\n",
            "epoch 6: w = 1.652 and loss = 0.18036\n",
            "epoch 7: w = 1.657 and loss = 0.17500\n",
            "epoch 8: w = 1.662 and loss = 0.16980\n",
            "epoch 9: w = 1.667 and loss = 0.16475\n",
            "epoch 10: w = 1.672 and loss = 0.15986\n",
            "epoch 11: w = 1.677 and loss = 0.15511\n",
            "epoch 12: w = 1.682 and loss = 0.15050\n",
            "epoch 13: w = 1.687 and loss = 0.14603\n",
            "epoch 14: w = 1.691 and loss = 0.14169\n",
            "epoch 15: w = 1.696 and loss = 0.13748\n",
            "epoch 16: w = 1.701 and loss = 0.13339\n",
            "epoch 17: w = 1.705 and loss = 0.12943\n",
            "epoch 18: w = 1.709 and loss = 0.12558\n",
            "epoch 19: w = 1.714 and loss = 0.12185\n",
            "epoch 20: w = 1.718 and loss = 0.11823\n",
            "\n",
            "Prediction after training: 9.419\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
