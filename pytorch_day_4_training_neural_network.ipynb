{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3svagen/Vj6A7K8fdxxlG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameshcodes/deep-learning-codes/blob/main/pytorch_day_4_training_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\textbf{Training Neural Network}$$\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "# $\\textbf{1. Loading a Dataset From TorchVision}$\n",
        "\n",
        "----\n"
      ],
      "metadata": {
        "id": "D2G2_HDnvzMC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J-ms2ek7vkiq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = datasets.FashionMNIST(\n",
        "    root='data',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "test = datasets.FashionMNIST(\n",
        "    root = 'data',\n",
        "    train = False,\n",
        "    download = False,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train, batch_size = 64, shuffle = True)\n",
        "test_dataloader = DataLoader(test, batch_size = 64, shuffle = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CuWJT_owEpK",
        "outputId": "5b61a489-b07c-4230-d222-1944c4122683"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 12735099.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 200340.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:03<00:00, 1252438.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 14698622.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $\\textbf{2. Build the Neural Network}$\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Al0YAG4NwoGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "QLOM1Gr9wu3X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using {device} device.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuP0xwgtwsme",
        "outputId": "559df4fc-9b86-460c-e3c8-a8344d871d78"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # First fully connected layer\n",
        "        self.fc1 = nn.Linear(28 * 28, 64)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "\n",
        "        # Second fully connected layer\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(32)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc3 = nn.Linear(32, 10)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "OvjsUsc2xOF_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vayQ6AxzRtg",
        "outputId": "8e5a7a3a-4213-4591-996e-8fbb6f5c3591"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
            "  (batch_norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (batch_norm2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
            "  (log_softmax): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "$\\text{$Note:$ To use the model, we pass it the input data. This executes the model’s forward, along with some background operations.}$\n",
        "\n",
        "Do not call `model.forward()` directly!\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For Example:\n"
      ],
      "metadata": {
        "id": "TlxMMOZb2FJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(1, 28, 28, device=device).to(device)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "logits = model(X)\n",
        "\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efnMki-T2Efe",
        "outputId": "907febd9-d97d-411a-9a94-0b3f9cad3da8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: tensor([9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $2.1$ $Model$ $Parameter$\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KxHDneUk5-gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0AR8Z_i54WI",
        "outputId": "5be8053a-7fc7-4777-d11e-334d893f9c9d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: fc1.weight | Size: torch.Size([64, 784]) | Values : tensor([[ 0.0133, -0.0237,  0.0206,  ...,  0.0139,  0.0252, -0.0354],\n",
            "        [-0.0050, -0.0281, -0.0206,  ...,  0.0111,  0.0132,  0.0084]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: fc1.bias | Size: torch.Size([64]) | Values : tensor([ 0.0268, -0.0006], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: batch_norm1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: batch_norm1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: fc2.weight | Size: torch.Size([32, 64]) | Values : tensor([[ 0.0019,  0.0728,  0.0160, -0.1096, -0.0224, -0.1016,  0.0629,  0.0023,\n",
            "         -0.0318,  0.0864,  0.0658, -0.0057,  0.0155, -0.1068,  0.1052,  0.0478,\n",
            "          0.0003,  0.0469,  0.0825, -0.1178,  0.0335,  0.1099, -0.0461,  0.0363,\n",
            "         -0.0271, -0.1058, -0.0787, -0.0416, -0.0631, -0.0724,  0.0916,  0.0602,\n",
            "          0.0233,  0.0690, -0.0083,  0.1036,  0.0065, -0.0189,  0.1199,  0.0454,\n",
            "          0.0544, -0.1127,  0.0043, -0.1151,  0.1050,  0.0700,  0.0925, -0.0890,\n",
            "         -0.0008, -0.1200, -0.0125,  0.0178,  0.0650, -0.1152, -0.0559,  0.0220,\n",
            "         -0.1085, -0.0349,  0.0330, -0.0973, -0.1045,  0.0824,  0.0176,  0.1096],\n",
            "        [-0.0652, -0.1249, -0.0899,  0.0767,  0.0002,  0.1235,  0.0282,  0.1186,\n",
            "         -0.0488, -0.0885, -0.0273, -0.0076,  0.1087, -0.0163, -0.0737, -0.0245,\n",
            "          0.0508,  0.1129,  0.0105,  0.1018,  0.0006, -0.0788,  0.0377,  0.1132,\n",
            "          0.0596,  0.1248,  0.0474, -0.0153,  0.0976, -0.0877,  0.1225,  0.0740,\n",
            "         -0.0599, -0.0799, -0.0545, -0.0079,  0.0904,  0.0138, -0.0899, -0.1099,\n",
            "          0.0926,  0.0102,  0.1100,  0.1142, -0.1231, -0.0663, -0.0130,  0.1188,\n",
            "          0.1075, -0.0121, -0.0019, -0.0869,  0.0669,  0.1094, -0.0763,  0.1072,\n",
            "          0.0011,  0.1139, -0.0787, -0.0785, -0.0028, -0.0438, -0.0646, -0.1246]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: fc2.bias | Size: torch.Size([32]) | Values : tensor([ 0.1089, -0.0058], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: batch_norm2.weight | Size: torch.Size([32]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: batch_norm2.bias | Size: torch.Size([32]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: fc3.weight | Size: torch.Size([10, 32]) | Values : tensor([[-0.0580,  0.0118,  0.1565,  0.1423,  0.1559, -0.0862, -0.1482, -0.0447,\n",
            "          0.0606, -0.0389, -0.1281, -0.0785, -0.0913,  0.1056, -0.0429, -0.0684,\n",
            "         -0.0510,  0.1359, -0.1145, -0.1421, -0.0729, -0.0591, -0.1006, -0.0510,\n",
            "          0.0481,  0.0198,  0.0051,  0.0505,  0.0573, -0.0503,  0.0762,  0.1356],\n",
            "        [-0.0300,  0.0282,  0.0766,  0.1016, -0.1567, -0.1183,  0.0667, -0.0469,\n",
            "          0.0613, -0.0572, -0.0381, -0.0983,  0.0880,  0.0207,  0.0545, -0.1021,\n",
            "         -0.0312,  0.1288,  0.1381, -0.1227, -0.1628,  0.0376, -0.1633,  0.1492,\n",
            "         -0.1732,  0.0043,  0.1088,  0.0327,  0.1151, -0.1695, -0.1612,  0.0220]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: fc3.bias | Size: torch.Size([10]) | Values : tensor([-0.1704,  0.0055], grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $\\textbf{3. Training the Neural Network}$\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Phsu2HpN5mHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "h7m7qFap3I7g"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$Hyperparameters$\n",
        "\n",
        "---\n",
        "**Number of Epochs** - the number times to iterate over the dataset\n",
        "\n",
        "**Batch Size** - the number of data samples propagated through the network before the parameters are updated\n",
        "\n",
        "**Learning Rate** - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training."
      ],
      "metadata": {
        "id": "fsFIN1I766tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-2\n",
        "batch_size = 64\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "8u3rw9aX693h"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhGl-UzL7gJx",
        "outputId": "1e10a70b-c891-4d84-cfaf-25aee4b6886e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.403424  [   64/60000]\n",
            "loss: 1.871727  [ 6464/60000]\n",
            "loss: 1.647410  [12864/60000]\n",
            "loss: 1.457512  [19264/60000]\n",
            "loss: 1.266085  [25664/60000]\n",
            "loss: 1.254902  [32064/60000]\n",
            "loss: 1.196556  [38464/60000]\n",
            "loss: 1.069652  [44864/60000]\n",
            "loss: 0.945549  [51264/60000]\n",
            "loss: 1.130857  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.4%, Avg loss: 0.733250 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.128931  [   64/60000]\n",
            "loss: 0.953985  [ 6464/60000]\n",
            "loss: 1.011680  [12864/60000]\n",
            "loss: 1.121553  [19264/60000]\n",
            "loss: 0.850140  [25664/60000]\n",
            "loss: 0.854010  [32064/60000]\n",
            "loss: 0.987238  [38464/60000]\n",
            "loss: 0.876438  [44864/60000]\n",
            "loss: 0.645891  [51264/60000]\n",
            "loss: 0.864375  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.3%, Avg loss: 0.577402 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.788171  [   64/60000]\n",
            "loss: 0.895216  [ 6464/60000]\n",
            "loss: 0.922458  [12864/60000]\n",
            "loss: 0.764669  [19264/60000]\n",
            "loss: 0.712194  [25664/60000]\n",
            "loss: 0.699676  [32064/60000]\n",
            "loss: 0.801616  [38464/60000]\n",
            "loss: 0.770086  [44864/60000]\n",
            "loss: 0.938368  [51264/60000]\n",
            "loss: 0.823784  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.526840 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.796466  [   64/60000]\n",
            "loss: 0.759876  [ 6464/60000]\n",
            "loss: 0.817097  [12864/60000]\n",
            "loss: 0.691408  [19264/60000]\n",
            "loss: 0.831178  [25664/60000]\n",
            "loss: 0.695415  [32064/60000]\n",
            "loss: 0.799874  [38464/60000]\n",
            "loss: 0.564862  [44864/60000]\n",
            "loss: 0.743367  [51264/60000]\n",
            "loss: 0.951825  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.6%, Avg loss: 0.501394 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.969864  [   64/60000]\n",
            "loss: 0.725214  [ 6464/60000]\n",
            "loss: 0.652587  [12864/60000]\n",
            "loss: 0.637361  [19264/60000]\n",
            "loss: 0.634947  [25664/60000]\n",
            "loss: 0.894176  [32064/60000]\n",
            "loss: 0.793189  [38464/60000]\n",
            "loss: 0.595767  [44864/60000]\n",
            "loss: 0.757404  [51264/60000]\n",
            "loss: 0.832493  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.7%, Avg loss: 0.510502 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.787181  [   64/60000]\n",
            "loss: 0.831775  [ 6464/60000]\n",
            "loss: 0.794665  [12864/60000]\n",
            "loss: 0.524652  [19264/60000]\n",
            "loss: 0.766177  [25664/60000]\n",
            "loss: 0.689831  [32064/60000]\n",
            "loss: 0.497791  [38464/60000]\n",
            "loss: 0.636874  [44864/60000]\n",
            "loss: 0.635430  [51264/60000]\n",
            "loss: 0.599131  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.7%, Avg loss: 0.473664 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.893317  [   64/60000]\n",
            "loss: 0.758152  [ 6464/60000]\n",
            "loss: 0.827978  [12864/60000]\n",
            "loss: 0.966175  [19264/60000]\n",
            "loss: 0.902710  [25664/60000]\n",
            "loss: 0.694128  [32064/60000]\n",
            "loss: 0.902490  [38464/60000]\n",
            "loss: 0.833808  [44864/60000]\n",
            "loss: 0.621190  [51264/60000]\n",
            "loss: 0.886014  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.465161 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.674655  [   64/60000]\n",
            "loss: 0.938332  [ 6464/60000]\n",
            "loss: 0.741013  [12864/60000]\n",
            "loss: 0.914318  [19264/60000]\n",
            "loss: 0.840800  [25664/60000]\n",
            "loss: 0.690782  [32064/60000]\n",
            "loss: 0.762585  [38464/60000]\n",
            "loss: 0.546272  [44864/60000]\n",
            "loss: 0.644407  [51264/60000]\n",
            "loss: 0.531907  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.454923 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.612985  [   64/60000]\n",
            "loss: 0.832934  [ 6464/60000]\n",
            "loss: 0.619218  [12864/60000]\n",
            "loss: 0.677596  [19264/60000]\n",
            "loss: 0.757038  [25664/60000]\n",
            "loss: 0.704848  [32064/60000]\n",
            "loss: 0.557946  [38464/60000]\n",
            "loss: 0.707464  [44864/60000]\n",
            "loss: 0.603824  [51264/60000]\n",
            "loss: 0.808535  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.471816 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.670089  [   64/60000]\n",
            "loss: 0.643128  [ 6464/60000]\n",
            "loss: 0.657306  [12864/60000]\n",
            "loss: 0.582476  [19264/60000]\n",
            "loss: 0.640583  [25664/60000]\n",
            "loss: 0.603284  [32064/60000]\n",
            "loss: 0.668890  [38464/60000]\n",
            "loss: 0.666434  [44864/60000]\n",
            "loss: 0.580611  [51264/60000]\n",
            "loss: 0.734449  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.449404 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $\\textbf{4. Saving Model}$\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "O9s91gHr8M4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'model.pth')"
      ],
      "metadata": {
        "id": "l5EcMZIF8ex6"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}